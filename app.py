import streamlit as st
import google.generativeai as genai
import chromadb

# è¨­å®š Google Gemini API
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
genai.configure(api_key=GEMINI_API_KEY)

# å»ºç«‹ Gemini 2.0 Flash æ¨¡å‹
model = genai.GenerativeModel("gemini-2.0-flash")

from FlagEmbedding import FlagModel
embedding_model = FlagModel(
    'chuxin-llm/Chuxin-Embedding',
    query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
    use_fp16=True  # ä½¿ç”¨åŠç²¾åº¦è¨ˆç®—ï¼ŒåŠ é€Ÿæ¨ç†
)

# åˆå§‹åŒ– ChromaDB
chroma_client = chromadb.EphemeralClient()
collection = chroma_client.get_or_create_collection(name="insurance_rag_test", embedding_function=None)

# åˆå§‹åŒ– Streamlit é é¢
st.title("ğŸ’¬ AI ä¿éšªé¡§å•")
st.write("ğŸ” èˆ‡ AI äº’å‹•ï¼Œè©¢å•å°ç£ä¿éšªç›¸é—œå•é¡Œã€‚")

# å»ºç«‹ Chat æ­·å²ï¼ˆåŒ…å« System Promptï¼‰
if "chat" not in st.session_state:
    st.session_state.chat = model.start_chat(history=[
        {
            "role": "model",
            "parts": [(
                "èƒŒæ™¯è¨­å®š: ä½ æ˜¯ä¸€ä½ç²¾é€šå°ç£ä¿éšªæ³•è¦èˆ‡ä¿éšªå•†å“çš„å°ˆæ¥­é¡§å•ã€‚\n"
                "è«‹æ ¹æ“šä½¿ç”¨è€…æä¾›çš„æª¢ç´¢å…§å®¹ï¼Œåƒ…åœ¨å…§å®¹ç¯„åœå…§å›ç­”å•é¡Œã€‚\n"
                "è‹¥è³‡è¨Šä¸è¶³æˆ–ç„¡æ³•å¾ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªæ˜ç„¡æ³•å›ç­”ã€‚\n"
                "è«‹ä½¿ç”¨æ¢åˆ—å¼æˆ–è¡¨æ ¼å›ç­”å•é¡Œï¼Œä»¥ç¢ºä¿æ¸…æ¥šæ˜“è®€ã€‚\n"
                "è«‹å‹¿æé€ è³‡è¨Šï¼Œè‹¥ç„¡æ³•å›ç­”ï¼Œè«‹æä¾›åˆç†å»ºè­°ã€‚\n"
            )]
        }
    ])

# ç”¨æˆ¶è¼¸å…¥å€
user_input = st.text_input("ğŸ’¡ ä½ çš„å•é¡Œï¼ˆä¾‹å¦‚ï¼šä»€éº¼æ˜¯å¼·åˆ¶éšªï¼Ÿï¼‰", "")



# å®šç¾©ä¸€å€‹å‡½æ•¸ä¾†ç”Ÿæˆå‘é‡
def get_chuxin_embedding(text):
    return embedding_model.encode([text])[0].tolist() 

if st.button("é€å‡º"):
    if user_input:
        # é€²è¡Œ RAG æŸ¥è©¢
        expanded_query = f"å°ç£ä¿éšªç›¸é—œå•é¡Œ: {user_input}"
        query_embedding = get_chuxin_embedding(expanded_query)
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=10  
        )
        
        retrieved_docs = "\n\n---\n\n".join([doc for doc in results["documents"][0]])
        print(retrieved_docs)
        # æ–°çš„ç”¨æˆ¶å•é¡Œ
        user_message = (
            f"ä»¥ä¸‹æ˜¯å¯èƒ½èˆ‡ã€{user_input}ã€ç›¸é—œçš„è³‡è¨Š (ä¾†è‡ªçŸ¥è­˜åº«æª¢ç´¢çµæœ)ï¼š\n\n"
            f"{retrieved_docs}\n\n"
            "è«‹æ ¹æ“šä»¥ä¸Šå…§å®¹å›ç­”ä¸‹åˆ—å•é¡Œï¼Œä¸¦åœ¨éœ€è¦æ™‚æä¾›å¿…è¦çš„ä¿éšªæˆ–æ³•è¦è§£é‡‹ã€‚\n"
            "å›ç­”æ™‚è¦å°‡çŸ¥è­˜åº«æª¢ç´¢çµæœç•¶æˆä½ æœ¬ä¾†å°±çŸ¥é“çš„å…§å®¹ï¼Œä¸¦å°‡å•é¡Œè¦–ç‚ºç„¡é—œä¹‹ç¬¬ä¸‰äººæ‰€å•çš„å•é¡Œã€‚\n\n"
            f"å•é¡Œ: {user_input}"
        )
        print(user_message)
        # ç™¼é€è¨Šæ¯åˆ° Chatï¼ˆä¿ç•™å°è©±ä¸Šä¸‹æ–‡ï¼‰
        chat = st.session_state.chat
        response = chat.send_message(user_message)

        # é¡¯ç¤ºå›æ‡‰
        st.markdown(f"### ğŸ¤– AI å›æ‡‰ï¼š\n{response.text}")

        # æ›´æ–°å°è©±ç´€éŒ„
        st.session_state.chat = chat
    else:
        st.warning("è«‹è¼¸å…¥å•é¡Œå¾Œå†é»æ“Šé€å‡ºï¼")

st.subheader("ğŸ’¬ èŠå¤©è¨˜éŒ„")

for msg in st.session_state.chat.history:
    role = "user" if msg.role == "user" else "assistant"
    if role == "assistant" and "èƒŒæ™¯è¨­å®š" in msg.parts[0].text:
        continue
    if role == "user":
        question_start = msg.parts[0].text.find("å•é¡Œ: ")
        if question_start != -1:
            msg_text = msg.parts[0].text[question_start:]
        else:
            msg_text = msg.parts[0].text
    else:
        msg_text = msg.parts[0].text
    with st.chat_message(role):
        st.markdown(msg_text)
    

